
Line no 1: TOKEN <COMMENT> Lexeme <> found

Line no 2: TOKEN <COMMENT> Lexeme < Created by subangkar on 5/2/18.> found

Line no 3: TOKEN <COMMENT> Lexeme <> found

<< Error @ Line no 6: Unrecognized character: # >>

Line no 6: TOKEN <ID> Lexeme define found
 ScopeTable # 0
 001 --> < define : ID > 


Line no 6: TOKEN <ID> Lexeme SYMBOL_TABLE_SIZE found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > 
 001 --> < define : ID > 


Line no 6: TOKEN <CONST_INT> Lexeme 73 found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > 
 001 --> < define : ID > 
 060 --> < 73 : CONST_INT > 


<< Error @ Line no 9: Unrecognized character: # >>

Line no 9: TOKEN <ID> Lexeme ifndef found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 060 --> < 73 : CONST_INT > 


Line no 9: TOKEN <ID> Lexeme LEXICALANALYZER_LEXBASE_H found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > 
 060 --> < 73 : CONST_INT > 


<< Error @ Line no 10: Unrecognized character: # >>

Line no 10: TOKEN <ID> Lexeme define found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > 
 060 --> < 73 : CONST_INT > 


Line no 10: TOKEN <ID> Lexeme LEXICALANALYZER_LEXBASE_H found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > 
 060 --> < 73 : CONST_INT > 


<< Error @ Line no 12: Unrecognized character: # >>

Line no 12: TOKEN <ID> Lexeme include found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > 
 060 --> < 73 : CONST_INT > 


Line no 12: TOKEN <STRING> Lexeme "DataStructure.h" found

<< Error @ Line no 13: Unrecognized character: # >>

Line no 13: TOKEN <ID> Lexeme include found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > 
 060 --> < 73 : CONST_INT > 


Line no 13: TOKEN <STRING> Lexeme "Utils.h" found

<< Error @ Line no 14: Unrecognized character: # >>

Line no 14: TOKEN <ID> Lexeme include found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > 
 060 --> < 73 : CONST_INT > 


Line no 14: TOKEN <RELOP> Lexeme < found

Line no 14: TOKEN <ID> Lexeme locale found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > 
 036 --> < locale : ID > 
 060 --> < 73 : CONST_INT > 


Line no 14: TOKEN <RELOP> Lexeme > found

Line no 16: TOKEN <ID> Lexeme SymbolTable found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > 
 022 --> < SymbolTable : ID > 
 036 --> < locale : ID > 
 060 --> < 73 : CONST_INT > 


Line no 16: TOKEN <ID> Lexeme hashTable found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > 
 022 --> < SymbolTable : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 060 --> < 73 : CONST_INT > 


Line no 16: TOKEN <LPAREN> Lexeme ( found

Line no 16: TOKEN <ID> Lexeme SYMBOL_TABLE_SIZE found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > 
 022 --> < SymbolTable : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 060 --> < 73 : CONST_INT > 


Line no 16: TOKEN <RPAREN> Lexeme ) found

Line no 16: TOKEN <SEMICOLON> Lexeme ; found

Line no 17: TOKEN <ID> Lexeme FILE found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > 
 022 --> < SymbolTable : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 054 --> < FILE : ID > 
 060 --> < 73 : CONST_INT > 


Line no 17: TOKEN <MULOP> Lexeme * found

Line no 17: TOKEN <ID> Lexeme logout found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 054 --> < FILE : ID > 
 060 --> < 73 : CONST_INT > 


Line no 17: TOKEN <COMMA> Lexeme , found

Line no 17: TOKEN <MULOP> Lexeme * found

Line no 17: TOKEN <ID> Lexeme tokenout found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 054 --> < FILE : ID > 
 060 --> < 73 : CONST_INT > 
 067 --> < tokenout : ID > 


Line no 17: TOKEN <SEMICOLON> Lexeme ; found

Line no 18: TOKEN <INT> Lexeme int found

Line no 18: TOKEN <ID> Lexeme line_count found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 054 --> < FILE : ID > 
 060 --> < 73 : CONST_INT > 
 067 --> < tokenout : ID > 


Line no 18: TOKEN <ASSIGNOP> Lexeme = found

Line no 18: TOKEN <CONST_INT> Lexeme 1 found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 054 --> < FILE : ID > 
 060 --> < 73 : CONST_INT > 
 067 --> < tokenout : ID > 


Line no 18: TOKEN <SEMICOLON> Lexeme ; found

Line no 19: TOKEN <INT> Lexeme int found

Line no 19: TOKEN <ID> Lexeme keyword_count found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 054 --> < FILE : ID > 
 060 --> < 73 : CONST_INT > 
 067 --> < tokenout : ID > 


Line no 19: TOKEN <ASSIGNOP> Lexeme = found

Line no 19: TOKEN <CONST_INT> Lexeme 0 found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 054 --> < FILE : ID > 
 060 --> < 73 : CONST_INT > 
 067 --> < tokenout : ID > 


Line no 19: TOKEN <SEMICOLON> Lexeme ; found

Line no 20: TOKEN <INT> Lexeme int found

Line no 20: TOKEN <ID> Lexeme err_count found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 054 --> < FILE : ID > 
 060 --> < 73 : CONST_INT > 
 067 --> < tokenout : ID > 


Line no 20: TOKEN <ASSIGNOP> Lexeme = found

Line no 20: TOKEN <CONST_INT> Lexeme 0 found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 054 --> < FILE : ID > 
 060 --> < 73 : CONST_INT > 
 067 --> < tokenout : ID > 


Line no 20: TOKEN <SEMICOLON> Lexeme ; found

<< Error @ Line no 24: Unrecognized character: # >>

Line no 24: TOKEN <ID> Lexeme define found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 054 --> < FILE : ID > 
 060 --> < 73 : CONST_INT > 
 067 --> < tokenout : ID > 


Line no 24: TOKEN <ID> Lexeme TOKEN_PRINT_KEY found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 054 --> < FILE : ID > 
 060 --> < 73 : CONST_INT > 
 067 --> < tokenout : ID > 


Line no 24: TOKEN <STRING> Lexeme "<%s> " found

<< Error @ Line no 25: Unrecognized character: # >>

Line no 25: TOKEN <ID> Lexeme define found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 054 --> < FILE : ID > 
 060 --> < 73 : CONST_INT > 
 067 --> < tokenout : ID > 


Line no 25: TOKEN <ID> Lexeme TOKEN_PRINT_SYMBOL found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 054 --> < FILE : ID > 
 060 --> < 73 : CONST_INT > 
 067 --> < tokenout : ID > 


Line no 25: TOKEN <STRING> Lexeme "<%s, %s> " found

<< Error @ Line no 26: Unrecognized character: # >>

Line no 26: TOKEN <ID> Lexeme define found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 054 --> < FILE : ID > 
 060 --> < 73 : CONST_INT > 
 067 --> < tokenout : ID > 


Line no 26: TOKEN <ID> Lexeme LOG_TOKEN_PRINT found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 054 --> < FILE : ID > 
 060 --> < 73 : CONST_INT > 
 067 --> < tokenout : ID > 


Line no 26: TOKEN <STRING> Lexeme "\nLine no %d: TOKEN <%s> Lexeme %s found\n" found

<< Error @ Line no 27: Unrecognized character: # >>

Line no 27: TOKEN <ID> Lexeme define found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 054 --> < FILE : ID > 
 060 --> < 73 : CONST_INT > 
 067 --> < tokenout : ID > 


Line no 27: TOKEN <ID> Lexeme LOG_ERROR_PRINT found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 054 --> < FILE : ID > 
 060 --> < 73 : CONST_INT > 
 067 --> < tokenout : ID > 


Line no 27: TOKEN <STRING> Lexeme "\n<< Error @ Line no %d: %s: %s >>\n" found

<< Error @ Line no 28: Unrecognized character: # >>

Line no 28: TOKEN <ID> Lexeme define found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 054 --> < FILE : ID > 
 060 --> < 73 : CONST_INT > 
 067 --> < tokenout : ID > 


Line no 28: TOKEN <ID> Lexeme LOG_COMMENT_PRINT found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 054 --> < FILE : ID > 
 060 --> < 73 : CONST_INT > 
 067 --> < tokenout : ID > 


Line no 28: TOKEN <STRING> Lexeme "\nLine no %d: TOKEN <COMMENT> Lexeme <%s> found\n" found

Line no 32: TOKEN <VOID> Lexeme void found

Line no 32: TOKEN <ID> Lexeme printLog found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 054 --> < FILE : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 


Line no 32: TOKEN <LPAREN> Lexeme ( found

Line no 32: TOKEN <INT> Lexeme int found

Line no 32: TOKEN <ID> Lexeme lineNo found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 054 --> < FILE : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 


Line no 32: TOKEN <COMMA> Lexeme , found

Line no 32: TOKEN <ID> Lexeme string found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > 
 028 --> < string : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 054 --> < FILE : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 


Line no 32: TOKEN <ID> Lexeme tokenName found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > 
 028 --> < string : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 054 --> < FILE : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 


Line no 32: TOKEN <COMMA> Lexeme , found

Line no 32: TOKEN <ID> Lexeme string found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > 
 028 --> < string : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 054 --> < FILE : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 


Line no 32: TOKEN <ID> Lexeme lexemeName found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > 
 028 --> < string : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 054 --> < FILE : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 


Line no 32: TOKEN <RPAREN> Lexeme ) found

Line no 32: TOKEN <LCURL> Lexeme { found

Line no 33: TOKEN <ID> Lexeme fprintf found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > 
 028 --> < string : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 


Line no 33: TOKEN <LPAREN> Lexeme ( found

Line no 33: TOKEN <ID> Lexeme logout found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > 
 028 --> < string : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 


Line no 33: TOKEN <COMMA> Lexeme , found

Line no 33: TOKEN <ID> Lexeme LOG_TOKEN_PRINT found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > 
 028 --> < string : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 


Line no 33: TOKEN <COMMA> Lexeme , found

Line no 33: TOKEN <ID> Lexeme line_count found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > 
 028 --> < string : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 


Line no 33: TOKEN <COMMA> Lexeme , found

Line no 33: TOKEN <ID> Lexeme tokenName found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > 
 028 --> < string : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 


<< Error @ Line no 33: Unrecognized character: . >>

Line no 33: TOKEN <ID> Lexeme data found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > 
 028 --> < string : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 


Line no 33: TOKEN <LPAREN> Lexeme ( found

Line no 33: TOKEN <RPAREN> Lexeme ) found

Line no 33: TOKEN <COMMA> Lexeme , found

Line no 33: TOKEN <ID> Lexeme lexemeName found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > 
 028 --> < string : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 


<< Error @ Line no 33: Unrecognized character: . >>

Line no 33: TOKEN <ID> Lexeme data found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > 
 028 --> < string : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 


Line no 33: TOKEN <LPAREN> Lexeme ( found

Line no 33: TOKEN <RPAREN> Lexeme ) found

Line no 33: TOKEN <RPAREN> Lexeme ) found

Line no 33: TOKEN <SEMICOLON> Lexeme ; found

Line no 34: TOKEN <RCURL> Lexeme } found

Line no 36: TOKEN <VOID> Lexeme void found

Line no 36: TOKEN <ID> Lexeme insertToHashTable found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > 
 028 --> < string : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 


Line no 36: TOKEN <LPAREN> Lexeme ( found

Line no 36: TOKEN <ID> Lexeme string found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > 
 028 --> < string : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 


Line no 36: TOKEN <ID> Lexeme token_symbol found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 


Line no 36: TOKEN <COMMA> Lexeme , found

Line no 36: TOKEN <ID> Lexeme string found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 


Line no 36: TOKEN <ID> Lexeme token_name found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 


Line no 36: TOKEN <RPAREN> Lexeme ) found

Line no 36: TOKEN <LCURL> Lexeme { found

Line no 37: TOKEN <ID> Lexeme hashTable found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 


<< Error @ Line no 37: Unrecognized character: . >>

Line no 37: TOKEN <ID> Lexeme insert found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 37: TOKEN <LPAREN> Lexeme ( found

Line no 37: TOKEN <ID> Lexeme token_symbol found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 37: TOKEN <COMMA> Lexeme , found

Line no 37: TOKEN <ID> Lexeme token_name found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 37: TOKEN <RPAREN> Lexeme ) found

Line no 37: TOKEN <SEMICOLON> Lexeme ; found

Line no 38: TOKEN <ID> Lexeme hashTable found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


<< Error @ Line no 38: Unrecognized character: . >>

Line no 38: TOKEN <ID> Lexeme printAllScope found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 38: TOKEN <LPAREN> Lexeme ( found

Line no 38: TOKEN <ID> Lexeme logout found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 38: TOKEN <RPAREN> Lexeme ) found

Line no 38: TOKEN <SEMICOLON> Lexeme ; found

Line no 40: TOKEN <RCURL> Lexeme } found

Line no 43: TOKEN <VOID> Lexeme void found

Line no 43: TOKEN <ID> Lexeme addToken_keyword found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 43: TOKEN <LPAREN> Lexeme ( found

Line no 43: TOKEN <ID> Lexeme string found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 43: TOKEN <ID> Lexeme token_name found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 43: TOKEN <RPAREN> Lexeme ) found

Line no 43: TOKEN <LCURL> Lexeme { found

Line no 44: TOKEN <ID> Lexeme fprintf found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 44: TOKEN <LPAREN> Lexeme ( found

Line no 44: TOKEN <ID> Lexeme tokenout found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 44: TOKEN <COMMA> Lexeme , found

Line no 44: TOKEN <ID> Lexeme TOKEN_PRINT_KEY found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 44: TOKEN <COMMA> Lexeme , found

Line no 44: TOKEN <ID> Lexeme token_name found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


<< Error @ Line no 44: Unrecognized character: . >>

Line no 44: TOKEN <ID> Lexeme data found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 44: TOKEN <LPAREN> Lexeme ( found

Line no 44: TOKEN <RPAREN> Lexeme ) found

Line no 44: TOKEN <RPAREN> Lexeme ) found

Line no 44: TOKEN <SEMICOLON> Lexeme ; found

Line no 45: TOKEN <ID> Lexeme printLog found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 45: TOKEN <LPAREN> Lexeme ( found

Line no 45: TOKEN <ID> Lexeme line_count found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 45: TOKEN <COMMA> Lexeme , found

Line no 45: TOKEN <ID> Lexeme token_name found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 45: TOKEN <COMMA> Lexeme , found

Line no 45: TOKEN <ID> Lexeme yytext found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 45: TOKEN <RPAREN> Lexeme ) found

Line no 45: TOKEN <SEMICOLON> Lexeme ; found

Line no 46: TOKEN <ID> Lexeme keyword_count found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 46: TOKEN <INCOP> Lexeme ++ found

Line no 46: TOKEN <SEMICOLON> Lexeme ; found

Line no 47: TOKEN <RCURL> Lexeme } found

Line no 49: TOKEN <VOID> Lexeme void found

Line no 49: TOKEN <ID> Lexeme addToken_keyword found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 49: TOKEN <LPAREN> Lexeme ( found

Line no 49: TOKEN <RPAREN> Lexeme ) found

Line no 49: TOKEN <LCURL> Lexeme { found

Line no 50: TOKEN <ID> Lexeme string found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 50: TOKEN <ID> Lexeme token_name found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 50: TOKEN <ASSIGNOP> Lexeme = found

Line no 50: TOKEN <ID> Lexeme StringParser found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


<< Error @ Line no 50: Unrecognized character: : >>

<< Error @ Line no 50: Unrecognized character: : >>

Line no 50: TOKEN <ID> Lexeme toUpperCase found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 50: TOKEN <LPAREN> Lexeme ( found

Line no 50: TOKEN <ID> Lexeme yytext found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 50: TOKEN <RPAREN> Lexeme ) found

Line no 50: TOKEN <SEMICOLON> Lexeme ; found

Line no 51: TOKEN <ID> Lexeme fprintf found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 51: TOKEN <LPAREN> Lexeme ( found

Line no 51: TOKEN <ID> Lexeme tokenout found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 51: TOKEN <COMMA> Lexeme , found

Line no 51: TOKEN <ID> Lexeme TOKEN_PRINT_KEY found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 51: TOKEN <COMMA> Lexeme , found

Line no 51: TOKEN <ID> Lexeme token_name found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


<< Error @ Line no 51: Unrecognized character: . >>

Line no 51: TOKEN <ID> Lexeme data found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 51: TOKEN <LPAREN> Lexeme ( found

Line no 51: TOKEN <RPAREN> Lexeme ) found

Line no 51: TOKEN <RPAREN> Lexeme ) found

Line no 51: TOKEN <SEMICOLON> Lexeme ; found

Line no 52: TOKEN <ID> Lexeme printLog found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 52: TOKEN <LPAREN> Lexeme ( found

Line no 52: TOKEN <ID> Lexeme line_count found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 52: TOKEN <COMMA> Lexeme , found

Line no 52: TOKEN <ID> Lexeme token_name found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 52: TOKEN <COMMA> Lexeme , found

Line no 52: TOKEN <ID> Lexeme yytext found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 52: TOKEN <RPAREN> Lexeme ) found

Line no 52: TOKEN <SEMICOLON> Lexeme ; found

Line no 53: TOKEN <ID> Lexeme keyword_count found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 53: TOKEN <INCOP> Lexeme ++ found

Line no 53: TOKEN <SEMICOLON> Lexeme ; found

Line no 54: TOKEN <RCURL> Lexeme } found

Line no 56: TOKEN <VOID> Lexeme void found

Line no 56: TOKEN <ID> Lexeme addToken_identifier found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 56: TOKEN <LPAREN> Lexeme ( found

Line no 56: TOKEN <RPAREN> Lexeme ) found

Line no 56: TOKEN <LCURL> Lexeme { found

Line no 57: TOKEN <ID> Lexeme string found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 57: TOKEN <ID> Lexeme token_name found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 57: TOKEN <ASSIGNOP> Lexeme = found

Line no 57: TOKEN <STRING> Lexeme "ID" found

Line no 57: TOKEN <SEMICOLON> Lexeme ; found

Line no 58: TOKEN <ID> Lexeme fprintf found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 58: TOKEN <LPAREN> Lexeme ( found

Line no 58: TOKEN <ID> Lexeme tokenout found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 58: TOKEN <COMMA> Lexeme , found

Line no 58: TOKEN <ID> Lexeme TOKEN_PRINT_SYMBOL found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 58: TOKEN <COMMA> Lexeme , found

Line no 58: TOKEN <ID> Lexeme token_name found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


<< Error @ Line no 58: Unrecognized character: . >>

Line no 58: TOKEN <ID> Lexeme data found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 58: TOKEN <LPAREN> Lexeme ( found

Line no 58: TOKEN <RPAREN> Lexeme ) found

Line no 58: TOKEN <COMMA> Lexeme , found

Line no 58: TOKEN <ID> Lexeme yytext found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 58: TOKEN <RPAREN> Lexeme ) found

Line no 58: TOKEN <SEMICOLON> Lexeme ; found

Line no 59: TOKEN <ID> Lexeme printLog found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 59: TOKEN <LPAREN> Lexeme ( found

Line no 59: TOKEN <ID> Lexeme line_count found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 59: TOKEN <COMMA> Lexeme , found

Line no 59: TOKEN <ID> Lexeme token_name found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 59: TOKEN <COMMA> Lexeme , found

Line no 59: TOKEN <ID> Lexeme yytext found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 59: TOKEN <RPAREN> Lexeme ) found

Line no 59: TOKEN <SEMICOLON> Lexeme ; found

Line no 60: TOKEN <ID> Lexeme insertToHashTable found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 60: TOKEN <LPAREN> Lexeme ( found

Line no 60: TOKEN <ID> Lexeme yytext found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 60: TOKEN <COMMA> Lexeme , found

Line no 60: TOKEN <ID> Lexeme token_name found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 60: TOKEN <RPAREN> Lexeme ) found

Line no 60: TOKEN <SEMICOLON> Lexeme ; found

Line no 61: TOKEN <RCURL> Lexeme } found

Line no 63: TOKEN <VOID> Lexeme void found

Line no 63: TOKEN <ID> Lexeme addToken_string found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 63: TOKEN <LPAREN> Lexeme ( found

Line no 63: TOKEN <RPAREN> Lexeme ) found

Line no 63: TOKEN <LCURL> Lexeme { found

Line no 64: TOKEN <ID> Lexeme string found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 64: TOKEN <ID> Lexeme token_name found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 64: TOKEN <ASSIGNOP> Lexeme = found

Line no 64: TOKEN <STRING> Lexeme "STRING" found

Line no 64: TOKEN <SEMICOLON> Lexeme ; found

Line no 66: TOKEN <ID> Lexeme string found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 66: TOKEN <ID> Lexeme string_literal found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 66: TOKEN <ASSIGNOP> Lexeme = found

Line no 66: TOKEN <ID> Lexeme StringParser found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


<< Error @ Line no 66: Unrecognized character: : >>

<< Error @ Line no 66: Unrecognized character: : >>

Line no 66: TOKEN <ID> Lexeme parse found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 66: TOKEN <LPAREN> Lexeme ( found

Line no 66: TOKEN <ID> Lexeme yytext found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 


Line no 66: TOKEN <RPAREN> Lexeme ) found

Line no 66: TOKEN <SEMICOLON> Lexeme ; found

Line no 67: TOKEN <ID> Lexeme StringUtils found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


<< Error @ Line no 67: Unrecognized character: : >>

<< Error @ Line no 67: Unrecognized character: : >>

Line no 67: TOKEN <ID> Lexeme replaceFirst found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 67: TOKEN <LPAREN> Lexeme ( found

Line no 67: TOKEN <ID> Lexeme string_literal found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 67: TOKEN <COMMA> Lexeme , found

Line no 67: TOKEN <STRING> Lexeme "\"" found

Line no 67: TOKEN <COMMA> Lexeme , found

Line no 67: TOKEN <STRING> Lexeme "" found

Line no 67: TOKEN <RPAREN> Lexeme ) found

Line no 67: TOKEN <SEMICOLON> Lexeme ; found

Line no 68: TOKEN <ID> Lexeme StringUtils found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


<< Error @ Line no 68: Unrecognized character: : >>

<< Error @ Line no 68: Unrecognized character: : >>

Line no 68: TOKEN <ID> Lexeme replaceLast found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 68: TOKEN <LPAREN> Lexeme ( found

Line no 68: TOKEN <ID> Lexeme string_literal found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 68: TOKEN <COMMA> Lexeme , found

Line no 68: TOKEN <STRING> Lexeme "\"" found

Line no 68: TOKEN <COMMA> Lexeme , found

Line no 68: TOKEN <STRING> Lexeme "" found

Line no 68: TOKEN <RPAREN> Lexeme ) found

Line no 68: TOKEN <SEMICOLON> Lexeme ; found

Line no 69: TOKEN <ID> Lexeme StringUtils found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


<< Error @ Line no 69: Unrecognized character: : >>

<< Error @ Line no 69: Unrecognized character: : >>

Line no 69: TOKEN <ID> Lexeme replaceAll found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 69: TOKEN <LPAREN> Lexeme ( found

Line no 69: TOKEN <ID> Lexeme string_literal found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 69: TOKEN <COMMA> Lexeme , found

Line no 69: TOKEN <STRING> Lexeme "\\\n" found

Line no 69: TOKEN <COMMA> Lexeme , found

Line no 69: TOKEN <STRING> Lexeme "" found

Line no 69: TOKEN <RPAREN> Lexeme ) found

Line no 69: TOKEN <SEMICOLON> Lexeme ; found

Line no 70: TOKEN <COMMENT> Lexeme <	string_literal=StringParser::parse(string_literal);> found

Line no 71: TOKEN <ID> Lexeme fprintf found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 71: TOKEN <LPAREN> Lexeme ( found

Line no 71: TOKEN <ID> Lexeme tokenout found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 71: TOKEN <COMMA> Lexeme , found

Line no 71: TOKEN <ID> Lexeme TOKEN_PRINT_SYMBOL found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 71: TOKEN <COMMA> Lexeme , found

Line no 71: TOKEN <ID> Lexeme token_name found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


<< Error @ Line no 71: Unrecognized character: . >>

Line no 71: TOKEN <ID> Lexeme data found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 71: TOKEN <LPAREN> Lexeme ( found

Line no 71: TOKEN <RPAREN> Lexeme ) found

Line no 71: TOKEN <COMMA> Lexeme , found

Line no 71: TOKEN <ID> Lexeme string_literal found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


<< Error @ Line no 71: Unrecognized character: . >>

Line no 71: TOKEN <ID> Lexeme data found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 71: TOKEN <LPAREN> Lexeme ( found

Line no 71: TOKEN <RPAREN> Lexeme ) found

Line no 71: TOKEN <RPAREN> Lexeme ) found

Line no 71: TOKEN <SEMICOLON> Lexeme ; found

Line no 72: TOKEN <ID> Lexeme printLog found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 72: TOKEN <LPAREN> Lexeme ( found

Line no 72: TOKEN <ID> Lexeme line_count found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 72: TOKEN <COMMA> Lexeme , found

Line no 72: TOKEN <ID> Lexeme token_name found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 72: TOKEN <COMMA> Lexeme , found

Line no 72: TOKEN <ID> Lexeme yytext found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 72: TOKEN <RPAREN> Lexeme ) found

Line no 72: TOKEN <SEMICOLON> Lexeme ; found

Line no 74: TOKEN <ID> Lexeme line_count found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 74: TOKEN <ADDOP> Lexeme + found

Line no 74: TOKEN <ASSIGNOP> Lexeme = found

Line no 74: TOKEN <ID> Lexeme StringUtils found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


<< Error @ Line no 74: Unrecognized character: : >>

<< Error @ Line no 74: Unrecognized character: : >>

Line no 74: TOKEN <ID> Lexeme occCount found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 74: TOKEN <LPAREN> Lexeme ( found

Line no 74: TOKEN <ID> Lexeme yytext found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 74: TOKEN <COMMA> Lexeme , found

Line no 74: TOKEN <CONST_CHAR> Lexeme '\n' found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 74: TOKEN <RPAREN> Lexeme ) found

Line no 74: TOKEN <SEMICOLON> Lexeme ; found

Line no 76: TOKEN <COMMENT> Lexeme <	insertToHashTable(string_literal,token_name);> found

Line no 77: TOKEN <RCURL> Lexeme } found

Line no 80: TOKEN <VOID> Lexeme void found

Line no 80: TOKEN <ID> Lexeme addToken_const_int found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 80: TOKEN <LPAREN> Lexeme ( found

Line no 80: TOKEN <RPAREN> Lexeme ) found

Line no 80: TOKEN <LCURL> Lexeme { found

Line no 81: TOKEN <ID> Lexeme string found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 81: TOKEN <ID> Lexeme token_name found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 81: TOKEN <ASSIGNOP> Lexeme = found

Line no 81: TOKEN <STRING> Lexeme "CONST_INT" found

Line no 81: TOKEN <SEMICOLON> Lexeme ; found

Line no 82: TOKEN <ID> Lexeme fprintf found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 82: TOKEN <LPAREN> Lexeme ( found

Line no 82: TOKEN <ID> Lexeme tokenout found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 82: TOKEN <COMMA> Lexeme , found

Line no 82: TOKEN <ID> Lexeme TOKEN_PRINT_SYMBOL found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 82: TOKEN <COMMA> Lexeme , found

Line no 82: TOKEN <ID> Lexeme token_name found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


<< Error @ Line no 82: Unrecognized character: . >>

Line no 82: TOKEN <ID> Lexeme data found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 82: TOKEN <LPAREN> Lexeme ( found

Line no 82: TOKEN <RPAREN> Lexeme ) found

Line no 82: TOKEN <COMMA> Lexeme , found

Line no 82: TOKEN <ID> Lexeme yytext found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 82: TOKEN <RPAREN> Lexeme ) found

Line no 82: TOKEN <SEMICOLON> Lexeme ; found

Line no 83: TOKEN <ID> Lexeme printLog found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 83: TOKEN <LPAREN> Lexeme ( found

Line no 83: TOKEN <ID> Lexeme line_count found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 83: TOKEN <COMMA> Lexeme , found

Line no 83: TOKEN <ID> Lexeme token_name found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 83: TOKEN <COMMA> Lexeme , found

Line no 83: TOKEN <ID> Lexeme yytext found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 83: TOKEN <RPAREN> Lexeme ) found

Line no 83: TOKEN <SEMICOLON> Lexeme ; found

Line no 85: TOKEN <ID> Lexeme insertToHashTable found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 85: TOKEN <LPAREN> Lexeme ( found

Line no 85: TOKEN <ID> Lexeme yytext found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 85: TOKEN <COMMA> Lexeme , found

Line no 85: TOKEN <ID> Lexeme token_name found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 85: TOKEN <RPAREN> Lexeme ) found

Line no 85: TOKEN <SEMICOLON> Lexeme ; found

Line no 86: TOKEN <RCURL> Lexeme } found

Line no 88: TOKEN <VOID> Lexeme void found

Line no 88: TOKEN <ID> Lexeme addToken_const_float found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 88: TOKEN <LPAREN> Lexeme ( found

Line no 88: TOKEN <RPAREN> Lexeme ) found

Line no 88: TOKEN <LCURL> Lexeme { found

Line no 89: TOKEN <ID> Lexeme string found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 89: TOKEN <ID> Lexeme token_name found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 89: TOKEN <ASSIGNOP> Lexeme = found

Line no 89: TOKEN <STRING> Lexeme "CONST_FLOAT" found

Line no 89: TOKEN <SEMICOLON> Lexeme ; found

Line no 90: TOKEN <ID> Lexeme fprintf found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 90: TOKEN <LPAREN> Lexeme ( found

Line no 90: TOKEN <ID> Lexeme tokenout found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 90: TOKEN <COMMA> Lexeme , found

Line no 90: TOKEN <ID> Lexeme TOKEN_PRINT_SYMBOL found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 90: TOKEN <COMMA> Lexeme , found

Line no 90: TOKEN <ID> Lexeme token_name found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


<< Error @ Line no 90: Unrecognized character: . >>

Line no 90: TOKEN <ID> Lexeme data found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 90: TOKEN <LPAREN> Lexeme ( found

Line no 90: TOKEN <RPAREN> Lexeme ) found

Line no 90: TOKEN <COMMA> Lexeme , found

Line no 90: TOKEN <ID> Lexeme yytext found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 90: TOKEN <RPAREN> Lexeme ) found

Line no 90: TOKEN <SEMICOLON> Lexeme ; found

Line no 91: TOKEN <ID> Lexeme printLog found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 91: TOKEN <LPAREN> Lexeme ( found

Line no 91: TOKEN <ID> Lexeme line_count found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 91: TOKEN <COMMA> Lexeme , found

Line no 91: TOKEN <ID> Lexeme token_name found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 91: TOKEN <COMMA> Lexeme , found

Line no 91: TOKEN <ID> Lexeme yytext found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 91: TOKEN <RPAREN> Lexeme ) found

Line no 91: TOKEN <SEMICOLON> Lexeme ; found

Line no 93: TOKEN <ID> Lexeme insertToHashTable found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 93: TOKEN <LPAREN> Lexeme ( found

Line no 93: TOKEN <ID> Lexeme yytext found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 93: TOKEN <COMMA> Lexeme , found

Line no 93: TOKEN <ID> Lexeme token_name found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 93: TOKEN <RPAREN> Lexeme ) found

Line no 93: TOKEN <SEMICOLON> Lexeme ; found

Line no 94: TOKEN <RCURL> Lexeme } found

Line no 96: TOKEN <VOID> Lexeme void found

Line no 96: TOKEN <ID> Lexeme addToken_const_char found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 96: TOKEN <LPAREN> Lexeme ( found

Line no 96: TOKEN <RPAREN> Lexeme ) found

Line no 96: TOKEN <LCURL> Lexeme { found

Line no 97: TOKEN <ID> Lexeme string found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 97: TOKEN <ID> Lexeme token_name found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 97: TOKEN <ASSIGNOP> Lexeme = found

Line no 97: TOKEN <STRING> Lexeme "CONST_CHAR" found

Line no 97: TOKEN <SEMICOLON> Lexeme ; found

Line no 98: TOKEN <ID> Lexeme string found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 98: TOKEN <ID> Lexeme char_literal found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 98: TOKEN <ASSIGNOP> Lexeme = found

Line no 98: TOKEN <ID> Lexeme StringParser found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


<< Error @ Line no 98: Unrecognized character: : >>

<< Error @ Line no 98: Unrecognized character: : >>

Line no 98: TOKEN <ID> Lexeme parse found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 98: TOKEN <LPAREN> Lexeme ( found

Line no 98: TOKEN <ID> Lexeme yytext found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 98: TOKEN <RPAREN> Lexeme ) found

Line no 98: TOKEN <SEMICOLON> Lexeme ; found

Line no 99: TOKEN <ID> Lexeme StringUtils found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


<< Error @ Line no 99: Unrecognized character: : >>

<< Error @ Line no 99: Unrecognized character: : >>

Line no 99: TOKEN <ID> Lexeme replaceFirst found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 99: TOKEN <LPAREN> Lexeme ( found

Line no 99: TOKEN <ID> Lexeme char_literal found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 99: TOKEN <COMMA> Lexeme , found

Line no 99: TOKEN <STRING> Lexeme "\'" found

Line no 99: TOKEN <COMMA> Lexeme , found

Line no 99: TOKEN <STRING> Lexeme "" found

Line no 99: TOKEN <RPAREN> Lexeme ) found

Line no 99: TOKEN <SEMICOLON> Lexeme ; found

Line no 100: TOKEN <ID> Lexeme StringUtils found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


<< Error @ Line no 100: Unrecognized character: : >>

<< Error @ Line no 100: Unrecognized character: : >>

Line no 100: TOKEN <ID> Lexeme replaceLast found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 100: TOKEN <LPAREN> Lexeme ( found

Line no 100: TOKEN <ID> Lexeme char_literal found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 100: TOKEN <COMMA> Lexeme , found

Line no 100: TOKEN <STRING> Lexeme "\'" found

Line no 100: TOKEN <COMMA> Lexeme , found

Line no 100: TOKEN <STRING> Lexeme "" found

Line no 100: TOKEN <RPAREN> Lexeme ) found

Line no 100: TOKEN <SEMICOLON> Lexeme ; found

Line no 102: TOKEN <ID> Lexeme fprintf found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 102: TOKEN <LPAREN> Lexeme ( found

Line no 102: TOKEN <ID> Lexeme tokenout found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 102: TOKEN <COMMA> Lexeme , found

Line no 102: TOKEN <ID> Lexeme TOKEN_PRINT_SYMBOL found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 102: TOKEN <COMMA> Lexeme , found

Line no 102: TOKEN <ID> Lexeme token_name found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


<< Error @ Line no 102: Unrecognized character: . >>

Line no 102: TOKEN <ID> Lexeme data found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 102: TOKEN <LPAREN> Lexeme ( found

Line no 102: TOKEN <RPAREN> Lexeme ) found

Line no 102: TOKEN <COMMA> Lexeme , found

Line no 102: TOKEN <ID> Lexeme char_literal found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


<< Error @ Line no 102: Unrecognized character: . >>

Line no 102: TOKEN <ID> Lexeme data found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 102: TOKEN <LPAREN> Lexeme ( found

Line no 102: TOKEN <RPAREN> Lexeme ) found

Line no 102: TOKEN <RPAREN> Lexeme ) found

Line no 102: TOKEN <SEMICOLON> Lexeme ; found

Line no 103: TOKEN <ID> Lexeme printLog found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 103: TOKEN <LPAREN> Lexeme ( found

Line no 103: TOKEN <ID> Lexeme line_count found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 103: TOKEN <COMMA> Lexeme , found

Line no 103: TOKEN <ID> Lexeme token_name found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 103: TOKEN <COMMA> Lexeme , found

Line no 103: TOKEN <ID> Lexeme yytext found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 103: TOKEN <RPAREN> Lexeme ) found

Line no 103: TOKEN <SEMICOLON> Lexeme ; found

Line no 105: TOKEN <ID> Lexeme insertToHashTable found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 105: TOKEN <LPAREN> Lexeme ( found

Line no 105: TOKEN <ID> Lexeme yytext found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 105: TOKEN <COMMA> Lexeme , found

Line no 105: TOKEN <ID> Lexeme token_name found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 105: TOKEN <RPAREN> Lexeme ) found

Line no 105: TOKEN <SEMICOLON> Lexeme ; found

Line no 106: TOKEN <RCURL> Lexeme } found

Line no 108: TOKEN <VOID> Lexeme void found

Line no 108: TOKEN <ID> Lexeme addToken_operator found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 108: TOKEN <LPAREN> Lexeme ( found

Line no 108: TOKEN <ID> Lexeme string found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 108: TOKEN <ID> Lexeme token_name found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 108: TOKEN <RPAREN> Lexeme ) found

Line no 108: TOKEN <LCURL> Lexeme { found

Line no 109: TOKEN <ID> Lexeme fprintf found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 109: TOKEN <LPAREN> Lexeme ( found

Line no 109: TOKEN <ID> Lexeme tokenout found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 109: TOKEN <COMMA> Lexeme , found

Line no 109: TOKEN <ID> Lexeme TOKEN_PRINT_SYMBOL found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 109: TOKEN <COMMA> Lexeme , found

Line no 109: TOKEN <ID> Lexeme token_name found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


<< Error @ Line no 109: Unrecognized character: . >>

Line no 109: TOKEN <ID> Lexeme data found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 109: TOKEN <LPAREN> Lexeme ( found

Line no 109: TOKEN <RPAREN> Lexeme ) found

Line no 109: TOKEN <COMMA> Lexeme , found

Line no 109: TOKEN <ID> Lexeme yytext found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 109: TOKEN <RPAREN> Lexeme ) found

Line no 109: TOKEN <SEMICOLON> Lexeme ; found

Line no 110: TOKEN <ID> Lexeme printLog found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 110: TOKEN <LPAREN> Lexeme ( found

Line no 110: TOKEN <ID> Lexeme line_count found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 110: TOKEN <COMMA> Lexeme , found

Line no 110: TOKEN <ID> Lexeme token_name found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 110: TOKEN <COMMA> Lexeme , found

Line no 110: TOKEN <ID> Lexeme yytext found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 110: TOKEN <RPAREN> Lexeme ) found

Line no 110: TOKEN <SEMICOLON> Lexeme ; found

Line no 112: TOKEN <COMMENT> Lexeme < insertToHashTable(yytext, token_name);> found

Line no 113: TOKEN <RCURL> Lexeme } found

Line no 116: TOKEN <VOID> Lexeme void found

Line no 116: TOKEN <ID> Lexeme printError found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 116: TOKEN <LPAREN> Lexeme ( found

Line no 116: TOKEN <ID> Lexeme string found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 116: TOKEN <ID> Lexeme msg found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 116: TOKEN <RPAREN> Lexeme ) found

Line no 116: TOKEN <LCURL> Lexeme { found

Line no 117: TOKEN <ID> Lexeme fprintf found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 117: TOKEN <LPAREN> Lexeme ( found

Line no 117: TOKEN <ID> Lexeme logout found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 117: TOKEN <COMMA> Lexeme , found

Line no 117: TOKEN <ID> Lexeme LOG_ERROR_PRINT found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 117: TOKEN <COMMA> Lexeme , found

Line no 117: TOKEN <ID> Lexeme line_count found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 117: TOKEN <COMMA> Lexeme , found

Line no 117: TOKEN <ID> Lexeme msg found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


<< Error @ Line no 117: Unrecognized character: . >>

Line no 117: TOKEN <ID> Lexeme data found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 117: TOKEN <LPAREN> Lexeme ( found

Line no 117: TOKEN <RPAREN> Lexeme ) found

Line no 117: TOKEN <COMMA> Lexeme , found

Line no 117: TOKEN <ID> Lexeme yytext found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 117: TOKEN <RPAREN> Lexeme ) found

Line no 117: TOKEN <SEMICOLON> Lexeme ; found

Line no 119: TOKEN <ID> Lexeme err_count found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 119: TOKEN <INCOP> Lexeme ++ found

Line no 119: TOKEN <SEMICOLON> Lexeme ; found

Line no 121: TOKEN <ID> Lexeme line_count found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 121: TOKEN <ADDOP> Lexeme + found

Line no 121: TOKEN <ASSIGNOP> Lexeme = found

Line no 121: TOKEN <ID> Lexeme StringUtils found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


<< Error @ Line no 121: Unrecognized character: : >>

<< Error @ Line no 121: Unrecognized character: : >>

Line no 121: TOKEN <ID> Lexeme occCount found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 121: TOKEN <LPAREN> Lexeme ( found

Line no 121: TOKEN <ID> Lexeme yytext found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 121: TOKEN <COMMA> Lexeme , found

Line no 121: TOKEN <CONST_CHAR> Lexeme '\n' found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 121: TOKEN <RPAREN> Lexeme ) found

Line no 121: TOKEN <SEMICOLON> Lexeme ; found

Line no 122: TOKEN <RCURL> Lexeme } found

Line no 124: TOKEN <VOID> Lexeme void found

Line no 124: TOKEN <ID> Lexeme comment found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 046 --> < comment : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 124: TOKEN <LPAREN> Lexeme ( found

Line no 124: TOKEN <RPAREN> Lexeme ) found

Line no 124: TOKEN <LCURL> Lexeme { found

Line no 126: TOKEN <ID> Lexeme string found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 046 --> < comment : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 072 --> < StringUtils : ID > 


Line no 126: TOKEN <ID> Lexeme cmnt found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 046 --> < comment : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 070 --> < cmnt : ID > 
 072 --> < StringUtils : ID > 


Line no 126: TOKEN <ASSIGNOP> Lexeme = found

Line no 126: TOKEN <ID> Lexeme string found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 046 --> < comment : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 070 --> < cmnt : ID > 
 072 --> < StringUtils : ID > 


Line no 126: TOKEN <LPAREN> Lexeme ( found

Line no 126: TOKEN <ID> Lexeme yytext found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 046 --> < comment : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 070 --> < cmnt : ID > 
 072 --> < StringUtils : ID > 


Line no 126: TOKEN <RPAREN> Lexeme ) found

Line no 126: TOKEN <SEMICOLON> Lexeme ; found

Line no 128: TOKEN <IF> Lexeme if found

Line no 128: TOKEN <LPAREN> Lexeme ( found

Line no 128: TOKEN <ID> Lexeme cmnt found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 046 --> < comment : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 070 --> < cmnt : ID > 
 072 --> < StringUtils : ID > 


Line no 128: TOKEN <LTHIRD> Lexeme [ found

Line no 128: TOKEN <CONST_INT> Lexeme 1 found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 046 --> < comment : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 070 --> < cmnt : ID > 
 072 --> < StringUtils : ID > 


Line no 128: TOKEN <RTHIRD> Lexeme ] found

Line no 128: TOKEN <RELOP> Lexeme == found

Line no 128: TOKEN <CONST_CHAR> Lexeme '/' found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 046 --> < comment : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 070 --> < cmnt : ID > 
 072 --> < StringUtils : ID > < '/' : CONST_CHAR > 


Line no 128: TOKEN <RPAREN> Lexeme ) found

Line no 128: TOKEN <LCURL> Lexeme { found

Line no 129: TOKEN <ID> Lexeme StringUtils found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 046 --> < comment : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 070 --> < cmnt : ID > 
 072 --> < StringUtils : ID > < '/' : CONST_CHAR > 


<< Error @ Line no 129: Unrecognized character: : >>

<< Error @ Line no 129: Unrecognized character: : >>

Line no 129: TOKEN <ID> Lexeme replaceFirst found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 046 --> < comment : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 070 --> < cmnt : ID > 
 072 --> < StringUtils : ID > < '/' : CONST_CHAR > 


Line no 129: TOKEN <LPAREN> Lexeme ( found

Line no 129: TOKEN <ID> Lexeme cmnt found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 046 --> < comment : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 070 --> < cmnt : ID > 
 072 --> < StringUtils : ID > < '/' : CONST_CHAR > 


Line no 129: TOKEN <COMMA> Lexeme , found

Line no 129: TOKEN <STRING> Lexeme "//" found

Line no 129: TOKEN <COMMA> Lexeme , found

Line no 129: TOKEN <STRING> Lexeme "" found

Line no 129: TOKEN <RPAREN> Lexeme ) found

Line no 129: TOKEN <SEMICOLON> Lexeme ; found

Line no 130: TOKEN <ID> Lexeme StringUtils found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 046 --> < comment : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 070 --> < cmnt : ID > 
 072 --> < StringUtils : ID > < '/' : CONST_CHAR > 


<< Error @ Line no 130: Unrecognized character: : >>

<< Error @ Line no 130: Unrecognized character: : >>

Line no 130: TOKEN <ID> Lexeme replaceAll found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 046 --> < comment : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 070 --> < cmnt : ID > 
 072 --> < StringUtils : ID > < '/' : CONST_CHAR > 


Line no 130: TOKEN <LPAREN> Lexeme ( found

Line no 130: TOKEN <ID> Lexeme cmnt found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 046 --> < comment : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 070 --> < cmnt : ID > 
 072 --> < StringUtils : ID > < '/' : CONST_CHAR > 


Line no 130: TOKEN <COMMA> Lexeme , found

Line no 130: TOKEN <STRING> Lexeme "\\\n" found

Line no 130: TOKEN <COMMA> Lexeme , found

Line no 130: TOKEN <STRING> Lexeme "" found

Line no 130: TOKEN <RPAREN> Lexeme ) found

Line no 130: TOKEN <SEMICOLON> Lexeme ; found

Line no 131: TOKEN <RCURL> Lexeme } found

Line no 131: TOKEN <ELSE> Lexeme else found

Line no 131: TOKEN <LCURL> Lexeme { found

Line no 132: TOKEN <ID> Lexeme StringUtils found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 046 --> < comment : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 070 --> < cmnt : ID > 
 072 --> < StringUtils : ID > < '/' : CONST_CHAR > 


<< Error @ Line no 132: Unrecognized character: : >>

<< Error @ Line no 132: Unrecognized character: : >>

Line no 132: TOKEN <ID> Lexeme replaceFirst found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 046 --> < comment : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 070 --> < cmnt : ID > 
 072 --> < StringUtils : ID > < '/' : CONST_CHAR > 


Line no 132: TOKEN <LPAREN> Lexeme ( found

Line no 132: TOKEN <ID> Lexeme cmnt found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 046 --> < comment : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 070 --> < cmnt : ID > 
 072 --> < StringUtils : ID > < '/' : CONST_CHAR > 


Line no 132: TOKEN <COMMA> Lexeme , found

Line no 132: TOKEN <STRING> Lexeme "/*" found

Line no 132: TOKEN <COMMA> Lexeme , found

Line no 132: TOKEN <STRING> Lexeme "" found

Line no 132: TOKEN <RPAREN> Lexeme ) found

Line no 132: TOKEN <SEMICOLON> Lexeme ; found

Line no 133: TOKEN <ID> Lexeme StringUtils found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 046 --> < comment : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 070 --> < cmnt : ID > 
 072 --> < StringUtils : ID > < '/' : CONST_CHAR > 


<< Error @ Line no 133: Unrecognized character: : >>

<< Error @ Line no 133: Unrecognized character: : >>

Line no 133: TOKEN <ID> Lexeme replaceFirst found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 046 --> < comment : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 070 --> < cmnt : ID > 
 072 --> < StringUtils : ID > < '/' : CONST_CHAR > 


Line no 133: TOKEN <LPAREN> Lexeme ( found

Line no 133: TOKEN <ID> Lexeme cmnt found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 046 --> < comment : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 070 --> < cmnt : ID > 
 072 --> < StringUtils : ID > < '/' : CONST_CHAR > 


Line no 133: TOKEN <COMMA> Lexeme , found

Line no 133: TOKEN <STRING> Lexeme "*/" found

Line no 133: TOKEN <COMMA> Lexeme , found

Line no 133: TOKEN <STRING> Lexeme "" found

Line no 133: TOKEN <RPAREN> Lexeme ) found

Line no 133: TOKEN <SEMICOLON> Lexeme ; found

Line no 134: TOKEN <RCURL> Lexeme } found

Line no 136: TOKEN <ID> Lexeme fprintf found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 046 --> < comment : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 070 --> < cmnt : ID > 
 072 --> < StringUtils : ID > < '/' : CONST_CHAR > 


Line no 136: TOKEN <LPAREN> Lexeme ( found

Line no 136: TOKEN <ID> Lexeme logout found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 046 --> < comment : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 070 --> < cmnt : ID > 
 072 --> < StringUtils : ID > < '/' : CONST_CHAR > 


Line no 136: TOKEN <COMMA> Lexeme , found

Line no 136: TOKEN <ID> Lexeme LOG_COMMENT_PRINT found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 046 --> < comment : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 070 --> < cmnt : ID > 
 072 --> < StringUtils : ID > < '/' : CONST_CHAR > 


Line no 136: TOKEN <COMMA> Lexeme , found

Line no 136: TOKEN <ID> Lexeme line_count found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 046 --> < comment : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 070 --> < cmnt : ID > 
 072 --> < StringUtils : ID > < '/' : CONST_CHAR > 


Line no 136: TOKEN <COMMA> Lexeme , found

Line no 136: TOKEN <ID> Lexeme cmnt found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 046 --> < comment : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 070 --> < cmnt : ID > 
 072 --> < StringUtils : ID > < '/' : CONST_CHAR > 


<< Error @ Line no 136: Unrecognized character: . >>

Line no 136: TOKEN <ID> Lexeme data found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 046 --> < comment : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 070 --> < cmnt : ID > 
 072 --> < StringUtils : ID > < '/' : CONST_CHAR > 


Line no 136: TOKEN <LPAREN> Lexeme ( found

Line no 136: TOKEN <RPAREN> Lexeme ) found

Line no 136: TOKEN <RPAREN> Lexeme ) found

Line no 136: TOKEN <SEMICOLON> Lexeme ; found

Line no 138: TOKEN <COMMENT> Lexeme <	string s(yytext);> found

Line no 139: TOKEN <ID> Lexeme line_count found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 046 --> < comment : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 070 --> < cmnt : ID > 
 072 --> < StringUtils : ID > < '/' : CONST_CHAR > 


Line no 139: TOKEN <ADDOP> Lexeme + found

Line no 139: TOKEN <ASSIGNOP> Lexeme = found

Line no 139: TOKEN <ID> Lexeme StringUtils found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 046 --> < comment : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 070 --> < cmnt : ID > 
 072 --> < StringUtils : ID > < '/' : CONST_CHAR > 


<< Error @ Line no 139: Unrecognized character: : >>

<< Error @ Line no 139: Unrecognized character: : >>

Line no 139: TOKEN <ID> Lexeme occCount found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 046 --> < comment : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 070 --> < cmnt : ID > 
 072 --> < StringUtils : ID > < '/' : CONST_CHAR > 


Line no 139: TOKEN <LPAREN> Lexeme ( found

Line no 139: TOKEN <ID> Lexeme yytext found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 046 --> < comment : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 070 --> < cmnt : ID > 
 072 --> < StringUtils : ID > < '/' : CONST_CHAR > 


Line no 139: TOKEN <COMMA> Lexeme , found

Line no 139: TOKEN <CONST_CHAR> Lexeme '\n' found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 046 --> < comment : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 070 --> < cmnt : ID > 
 072 --> < StringUtils : ID > < '/' : CONST_CHAR > 


Line no 139: TOKEN <RPAREN> Lexeme ) found

Line no 139: TOKEN <SEMICOLON> Lexeme ; found

Line no 140: TOKEN <RCURL> Lexeme } found

<< Error @ Line no 142: Unrecognized character: # >>

Line no 142: TOKEN <ID> Lexeme endif found
 ScopeTable # 0
 000 --> < SYMBOL_TABLE_SIZE : ID > < TOKEN_PRINT_KEY : ID > < TOKEN_PRINT_SYMBOL : ID > < LOG_TOKEN_PRINT : ID > < LOG_ERROR_PRINT : ID > < LOG_COMMENT_PRINT : ID > < insertToHashTable : ID > < addToken_keyword : ID > < StringParser : ID > < addToken_string : ID > 
 001 --> < define : ID > 
 003 --> < ifndef : ID > 
 004 --> < replaceAll : ID > 
 006 --> < msg : ID > 
 007 --> < include : ID > 
 010 --> < keyword_count : ID > 
 011 --> < LEXICALANALYZER_LEXBASE_H : ID > < 0 : CONST_INT > 
 013 --> < lexemeName : ID > 
 015 --> < data : ID > 
 016 --> < replaceFirst : ID > 
 017 --> < yytext : ID > 
 019 --> < tokenName : ID > < parse : ID > 
 021 --> < 1 : CONST_INT > 
 022 --> < SymbolTable : ID > 
 024 --> < logout : ID > 
 026 --> < line_count : ID > < token_symbol : ID > 
 027 --> < replaceLast : ID > 
 028 --> < string : ID > 
 029 --> < toUpperCase : ID > 
 031 --> < printAllScope : ID > 
 032 --> < lineNo : ID > 
 036 --> < locale : ID > 
 038 --> < hashTable : ID > < string_literal : ID > < endif : ID > 
 041 --> < occCount : ID > 
 043 --> < addToken_const_char : ID > 
 046 --> < comment : ID > 
 052 --> < err_count : ID > < printError : ID > 
 053 --> < fprintf : ID > 
 054 --> < FILE : ID > 
 059 --> < token_name : ID > < addToken_identifier : ID > < addToken_const_int : ID > < addToken_const_float : ID > < addToken_operator : ID > 
 060 --> < 73 : CONST_INT > < printLog : ID > 
 061 --> < '\n' : CONST_CHAR > 
 067 --> < tokenout : ID > < char_literal : ID > 
 068 --> < insert : ID > 
 070 --> < cmnt : ID > 
 072 --> < StringUtils : ID > < '/' : CONST_CHAR > 


Line no 142: TOKEN <COMMENT> Lexeme <LEXICALANALYZER_LEXBASE_H> found

Total Line Number: 143
Total Errors: 58
